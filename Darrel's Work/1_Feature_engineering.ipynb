{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Darrel_Dataset/train.csv\")\n",
    "test = pd.read_csv(\"Darrel_Dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307507, 238), (48744, 237))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first part of feature engineering. Here, we are gonna explore some feature that can be generated by multiplying, dividing, subtracting, and adding 2 (or more) column(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's look at the correlation matrix of the train and test dataset that we produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(200,200))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by doing the same thing as Will have done, which is to make polynomial feature using `EXT_SOURCE` and `DAYS_BIRTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_feature(df1, df2, columns, target, degree, save):\n",
    "    # Make a new dataframe for polynomial features\n",
    "    poly_features = df1[columns + [target]]\n",
    "    poly_features_test = df2[columns]\n",
    "\n",
    "    # imputer for handling missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    poly_target = poly_features[target]\n",
    "\n",
    "    poly_features = poly_features.drop(columns=[target])\n",
    "\n",
    "    # Need to impute missing values\n",
    "    poly_features = imputer.fit_transform(poly_features)\n",
    "    poly_features_test = imputer.transform(poly_features_test)\n",
    "                                    \n",
    "    # Create the polynomial object with specified degree\n",
    "    poly_transformer = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Train the polynomial features\n",
    "    poly_transformer.fit(poly_features)\n",
    "\n",
    "    # Transform the features\n",
    "    poly_features = poly_transformer.transform(poly_features)\n",
    "    poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "\n",
    "    # Create a dataframe of the features \n",
    "    poly_features = pd.DataFrame(poly_features, \n",
    "                                columns=poly_transformer.get_feature_names_out(columns))\n",
    "\n",
    "    poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                columns=poly_transformer.get_feature_names_out(columns))\n",
    "\n",
    "    # Add in the target\n",
    "    poly_features[target] = poly_target\n",
    "\n",
    "    print(\"Poly features shape is\", poly_features.shape)\n",
    "\n",
    "    # Find the correlations with the target\n",
    "    poly_corrs = poly_features.corr()[target].sort_values()\n",
    "    print(\"Top 20 correlations with the target:\")\n",
    "    print(poly_corrs.head(20))\n",
    "    print(\"\\nBottom 20 correlations with the target:\")\n",
    "    print(poly_corrs.tail(20))\n",
    "\n",
    "\n",
    "        # Merge polynomial features into training dataframe\n",
    "    poly_features['SK_ID_CURR'] = df1['SK_ID_CURR']\n",
    "    app_train_poly = df1.merge(poly_features, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Merge polynomial features into testing dataframe\n",
    "    poly_features_test['SK_ID_CURR'] = df2['SK_ID_CURR']\n",
    "    app_test_poly = df2.merge(poly_features_test, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Align the dataframes\n",
    "    app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)\n",
    "\n",
    "    app_train_poly[target] = poly_target\n",
    "\n",
    "    # Print out the new shapes\n",
    "    print('\\nTraining data with polynomial features shape: ', app_train_poly.shape)\n",
    "    print('Testing data with polynomial features shape:  ', app_test_poly.shape)\n",
    "\n",
    "    if save:\n",
    "        return app_train_poly, app_test_poly\n",
    "    else:\n",
    "        return df1, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_operations_to_columns(df1, df2, columns):\n",
    "    new_df1_addition = df1[columns].copy()\n",
    "    new_df2_addition = df2[columns].copy()\n",
    "\n",
    "    new_df1_subtraction = df1[columns].copy()\n",
    "    new_df2_subtraction = df2[columns].copy()\n",
    "\n",
    "    new_df1_division = df1[columns].copy()\n",
    "    new_df2_division = df2[columns].copy()\n",
    "\n",
    "\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                new_col1 = f\"{col1}_plus_{col2}\"\n",
    "                new_col2 = f\"{col1}_plus_{col2}\"\n",
    "                new_df1_addition[new_col1] = df1[col1] + df1[col2]\n",
    "                new_df2_addition[new_col2] = df2[col1] + df2[col2]\n",
    "    \n",
    "    new_df1_addition[\"TARGET\"] = df1[\"TARGET\"]\n",
    "    new_df1_addition_corr = new_df1_addition.corr()[\"TARGET\"].sort_values()\n",
    "\n",
    "\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                new_col1 = f\"{col1}_minus_{col2}\"\n",
    "                new_col2 = f\"{col1}_minus_{col2}\"\n",
    "                new_df1_subtraction[new_col1] = df1[col1] - df1[col2]\n",
    "                new_df2_subtraction[new_col2] = df2[col1] - df2[col2]\n",
    "    \n",
    "    new_df1_subtraction[\"TARGET\"] = df1[\"TARGET\"]\n",
    "    new_df1_subtraction_corr = new_df1_subtraction.corr()[\"TARGET\"].sort_values()\n",
    "\n",
    "\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                new_col1 = f\"{col1}_divided_by_{col2}\"\n",
    "                new_col2 = f\"{col1}_divided_by_{col2}\"\n",
    "                new_df1_division[new_col1] = df1[col1] / df1[col2]\n",
    "                new_df2_division[new_col2] = df2[col1] / df2[col2]\n",
    "\n",
    "    new_df1_division[\"TARGET\"] = df1[\"TARGET\"]\n",
    "    new_df1_division_corr = new_df1_division.corr()[\"TARGET\"].sort_values()\n",
    "\n",
    "    correlations = pd.concat([new_df1_addition_corr.to_frame(), new_df1_subtraction_corr.to_frame(), new_df1_division_corr.to_frame()])\n",
    "    corr_filtered = pd.concat([correlations.head(20), correlations.tail(20)])\n",
    "\n",
    "\n",
    "    return corr_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see polynomial feature correlation with target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly features shape is (307507, 57)\n",
      "Top 20 correlations with the target:\n",
      "EXT_SOURCE_2 EXT_SOURCE_3                -0.193943\n",
      "EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3   -0.189608\n",
      "EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH     -0.181288\n",
      "EXT_SOURCE_2^2 EXT_SOURCE_3              -0.176431\n",
      "EXT_SOURCE_2 EXT_SOURCE_3^2              -0.172287\n",
      "EXT_SOURCE_1 EXT_SOURCE_2                -0.166625\n",
      "EXT_SOURCE_1 EXT_SOURCE_3                -0.164070\n",
      "EXT_SOURCE_2                             -0.160294\n",
      "EXT_SOURCE_2 DAYS_BIRTH                  -0.156874\n",
      "EXT_SOURCE_1 EXT_SOURCE_2^2              -0.156867\n",
      "EXT_SOURCE_3                             -0.155899\n",
      "EXT_SOURCE_1 EXT_SOURCE_2 DAYS_BIRTH     -0.155892\n",
      "EXT_SOURCE_1 EXT_SOURCE_3 DAYS_BIRTH     -0.151820\n",
      "EXT_SOURCE_1 EXT_SOURCE_3^2              -0.150827\n",
      "EXT_SOURCE_3 DAYS_BIRTH                  -0.150114\n",
      "EXT_SOURCE_2^2                           -0.149512\n",
      "EXT_SOURCE_2^2 DAYS_BIRTH                -0.149314\n",
      "EXT_SOURCE_3^2 DAYS_BIRTH                -0.141782\n",
      "EXT_SOURCE_3^2                           -0.141667\n",
      "EXT_SOURCE_2^3                           -0.140243\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "Bottom 20 correlations with the target:\n",
      "EXT_SOURCE_2 DAYS_BIRTH DAYS_EMPLOYED   -0.094252\n",
      "EXT_SOURCE_1^2                          -0.091035\n",
      "EXT_SOURCE_1^3                          -0.083006\n",
      "EXT_SOURCE_1 DAYS_BIRTH DAYS_EMPLOYED   -0.078497\n",
      "DAYS_BIRTH                              -0.078242\n",
      "EXT_SOURCE_1^2 DAYS_EMPLOYED            -0.077874\n",
      "EXT_SOURCE_1 DAYS_EMPLOYED              -0.076879\n",
      "DAYS_BIRTH^2                            -0.076674\n",
      "DAYS_BIRTH^3                            -0.074275\n",
      "DAYS_BIRTH^2 DAYS_EMPLOYED              -0.070498\n",
      "DAYS_BIRTH DAYS_EMPLOYED                -0.069319\n",
      "DAYS_EMPLOYED                           -0.063366\n",
      "EXT_SOURCE_3 DAYS_EMPLOYED^2            -0.057602\n",
      "EXT_SOURCE_2 DAYS_EMPLOYED^2            -0.056379\n",
      "EXT_SOURCE_1 DAYS_EMPLOYED^2            -0.050215\n",
      "DAYS_EMPLOYED^2                         -0.046232\n",
      "DAYS_BIRTH DAYS_EMPLOYED^2              -0.045006\n",
      "DAYS_EMPLOYED^3                         -0.035227\n",
      "TARGET                                   1.000000\n",
      "1                                             NaN\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "Training data with polynomial features shape:  (307507, 294)\n",
      "Testing data with polynomial features shape:   (48744, 293)\n"
     ]
    }
   ],
   "source": [
    "columns_list = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
    "target_col = 'TARGET'\n",
    "\n",
    "train, test = polynomial_feature(train, test, columns_list, target_col, degree=3, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the features that we have made by adding, subtracting, and dividing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_plus_EXT_SOURCE_2</th>\n",
       "      <td>-0.223863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_plus_EXT_SOURCE_3</th>\n",
       "      <td>-0.223863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1_plus_EXT_SOURCE_3</th>\n",
       "      <td>-0.212388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_plus_EXT_SOURCE_1</th>\n",
       "      <td>-0.212388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1_plus_EXT_SOURCE_2</th>\n",
       "      <td>-0.194887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_plus_EXT_SOURCE_1</th>\n",
       "      <td>-0.194887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <td>-0.178926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <td>-0.160471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <td>-0.155317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_plus_DAYS_EMPLOYED</th>\n",
       "      <td>-0.083362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_plus_DAYS_BIRTH</th>\n",
       "      <td>-0.083362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_plus_DAYS_BIRTH</th>\n",
       "      <td>-0.081483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_plus_EXT_SOURCE_3</th>\n",
       "      <td>-0.081483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_plus_DAYS_BIRTH</th>\n",
       "      <td>-0.080591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_plus_EXT_SOURCE_2</th>\n",
       "      <td>-0.080591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_plus_DAYS_EMPLOYED</th>\n",
       "      <td>-0.079971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_plus_EXT_SOURCE_3</th>\n",
       "      <td>-0.079971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_plus_EXT_SOURCE_2</th>\n",
       "      <td>-0.079738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_plus_DAYS_EMPLOYED</th>\n",
       "      <td>-0.079738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <td>-0.078242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <td>-0.078242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <td>-0.074957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_divided_by_DAYS_BIRTH</th>\n",
       "      <td>-0.067952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1_divided_by_DAYS_EMPLOYED</th>\n",
       "      <td>-0.010866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_divided_by_DAYS_EMPLOYED</th>\n",
       "      <td>-0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_divided_by_DAYS_EMPLOYED</th>\n",
       "      <td>-0.002821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1_divided_by_EXT_SOURCE_2</th>\n",
       "      <td>-0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_divided_by_EXT_SOURCE_2</th>\n",
       "      <td>-0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_divided_by_EXT_SOURCE_2</th>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_divided_by_EXT_SOURCE_2</th>\n",
       "      <td>0.001159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_divided_by_EXT_SOURCE_3</th>\n",
       "      <td>0.011954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_1_divided_by_EXT_SOURCE_3</th>\n",
       "      <td>0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_divided_by_DAYS_EMPLOYED</th>\n",
       "      <td>0.014038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_divided_by_EXT_SOURCE_3</th>\n",
       "      <td>0.014192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_divided_by_EXT_SOURCE_3</th>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3_divided_by_EXT_SOURCE_1</th>\n",
       "      <td>0.039541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_EMPLOYED_divided_by_EXT_SOURCE_1</th>\n",
       "      <td>0.043764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2_divided_by_EXT_SOURCE_1</th>\n",
       "      <td>0.066586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAYS_BIRTH_divided_by_EXT_SOURCE_1</th>\n",
       "      <td>0.151298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TARGET</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TARGET\n",
       "EXT_SOURCE_3_plus_EXT_SOURCE_2        -0.223863\n",
       "EXT_SOURCE_2_plus_EXT_SOURCE_3        -0.223863\n",
       "EXT_SOURCE_1_plus_EXT_SOURCE_3        -0.212388\n",
       "EXT_SOURCE_3_plus_EXT_SOURCE_1        -0.212388\n",
       "EXT_SOURCE_1_plus_EXT_SOURCE_2        -0.194887\n",
       "EXT_SOURCE_2_plus_EXT_SOURCE_1        -0.194887\n",
       "EXT_SOURCE_3                          -0.178926\n",
       "EXT_SOURCE_2                          -0.160471\n",
       "EXT_SOURCE_1                          -0.155317\n",
       "DAYS_BIRTH_plus_DAYS_EMPLOYED         -0.083362\n",
       "DAYS_EMPLOYED_plus_DAYS_BIRTH         -0.083362\n",
       "EXT_SOURCE_3_plus_DAYS_BIRTH          -0.081483\n",
       "DAYS_BIRTH_plus_EXT_SOURCE_3          -0.081483\n",
       "EXT_SOURCE_2_plus_DAYS_BIRTH          -0.080591\n",
       "DAYS_BIRTH_plus_EXT_SOURCE_2          -0.080591\n",
       "EXT_SOURCE_3_plus_DAYS_EMPLOYED       -0.079971\n",
       "DAYS_EMPLOYED_plus_EXT_SOURCE_3       -0.079971\n",
       "DAYS_EMPLOYED_plus_EXT_SOURCE_2       -0.079738\n",
       "EXT_SOURCE_2_plus_DAYS_EMPLOYED       -0.079738\n",
       "DAYS_BIRTH                            -0.078242\n",
       "DAYS_BIRTH                            -0.078242\n",
       "DAYS_EMPLOYED                         -0.074957\n",
       "DAYS_EMPLOYED_divided_by_DAYS_BIRTH   -0.067952\n",
       "EXT_SOURCE_1_divided_by_DAYS_EMPLOYED -0.010866\n",
       "EXT_SOURCE_3_divided_by_DAYS_EMPLOYED -0.003913\n",
       "EXT_SOURCE_2_divided_by_DAYS_EMPLOYED -0.002821\n",
       "EXT_SOURCE_1_divided_by_EXT_SOURCE_2  -0.000697\n",
       "DAYS_EMPLOYED_divided_by_EXT_SOURCE_2 -0.000211\n",
       "DAYS_BIRTH_divided_by_EXT_SOURCE_2     0.000189\n",
       "EXT_SOURCE_3_divided_by_EXT_SOURCE_2   0.001159\n",
       "DAYS_EMPLOYED_divided_by_EXT_SOURCE_3  0.011954\n",
       "EXT_SOURCE_1_divided_by_EXT_SOURCE_3   0.012034\n",
       "DAYS_BIRTH_divided_by_DAYS_EMPLOYED    0.014038\n",
       "EXT_SOURCE_2_divided_by_EXT_SOURCE_3   0.014192\n",
       "DAYS_BIRTH_divided_by_EXT_SOURCE_3     0.018300\n",
       "EXT_SOURCE_3_divided_by_EXT_SOURCE_1   0.039541\n",
       "DAYS_EMPLOYED_divided_by_EXT_SOURCE_1  0.043764\n",
       "EXT_SOURCE_2_divided_by_EXT_SOURCE_1   0.066586\n",
       "DAYS_BIRTH_divided_by_EXT_SOURCE_1     0.151298\n",
       "TARGET                                 1.000000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_operations_to_columns(train, test, ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We've done some feature engineering with `DAYS` and `EXT_SOURCE` columns. Now we can choose which columns should we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"EXT_SOURCE_3_plus_EXT_SOURCE_2\"] = train[\"EXT_SOURCE_3\"] + train[\"EXT_SOURCE_2\"]\n",
    "train[\"EXT_SOURCE_1_plus_EXT_SOURCE_3\"] = train[\"EXT_SOURCE_1\"] + train[\"EXT_SOURCE_3\"]\n",
    "train[\"EXT_SOURCE_1_plus_EXT_SOURCE_2\"] = train[\"EXT_SOURCE_1\"] + train[\"EXT_SOURCE_2\"]\n",
    "train[\"DAYS_BIRTH_divided_by_EXT_SOURCE_1\"] = train[\"DAYS_BIRTH\"] / train[\"EXT_SOURCE_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"EXT_SOURCE_3_plus_EXT_SOURCE_2\"] = test[\"EXT_SOURCE_3\"] + test[\"EXT_SOURCE_2\"]\n",
    "test[\"EXT_SOURCE_1_plus_EXT_SOURCE_3\"] = test[\"EXT_SOURCE_1\"] + test[\"EXT_SOURCE_3\"]\n",
    "test[\"EXT_SOURCE_1_plus_EXT_SOURCE_2\"] = test[\"EXT_SOURCE_1\"] + test[\"EXT_SOURCE_2\"]\n",
    "test[\"DAYS_BIRTH_divided_by_EXT_SOURCE_1\"] = test[\"DAYS_BIRTH\"] / test[\"EXT_SOURCE_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple experiments, these are the columns that produce good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's try to do feature engineering using domain knowledge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "train['ANNUITY_INCOME_PERCENT'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "train['CREDIT_TERM'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\n",
    "train['DAYS_EMPLOYED_PERCENT'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "\n",
    "test['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "test['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "test['CREDIT_TERM'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']\n",
    "test['DAYS_EMPLOYED_PERCENT'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label = train[\"TARGET\"]\n",
    "train = train.drop(\"TARGET\", axis=1)\n",
    "train, test = train.align(test, join='inner', axis=1)\n",
    "train[\"TARGET\"] = y_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"Darrel_Dataset/train_fe_little_domain.csv\", index=False)\n",
    "# test.to_csv(\"Darrel_Dataset/test_fe_little_domain.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is getting considerably higher score, this means that we are on the right path. Next up, we want to do another feature engineering that is often used by kaggle grandmaster which is column aggregation. See you on the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
